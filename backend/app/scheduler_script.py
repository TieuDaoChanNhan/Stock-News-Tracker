import requests
import json
import time
import schedule
from datetime import datetime
from typing import List, Dict, Optional
import sys
import os

# ThÃªm thÆ° má»¥c app vÃ o Python path
sys.path.append(os.path.dirname(__file__))

# Import crawler (KHÃ”NG import AI cÅ© ná»¯a)
from services.generic_crawler import scrape_news_from_website

API_BASE_URL = "http://127.0.0.1:8000/api/v1"

def post_article_to_api(article_data: dict) -> Optional[Dict]:
    """Gá»­i bÃ i bÃ¡o Ä‘Ã£ crawl lÃªn API Ä‘á»ƒ lÆ°u trá»¯."""
    payload = {
        "title": article_data.get("title"),
        "url": article_data.get("url"),
        "summary": article_data.get("summary"),
        "published_date_str": article_data.get("published_date_str") or article_data.get("collected_at_iso"),
        "source_url": article_data.get("source_page")
    }
    
    payload = {k: v for k, v in payload.items() if v is not None}

    try:
        response = requests.post(f"{API_BASE_URL}/articles/", json=payload)
        response.raise_for_status()
        
        created_article = response.json()
        print(f"âœ… Posted article to DB: '{payload.get('title')[:50]}...' (ID: {created_article.get('id')})")
        return created_article
        
    except Exception as e:
        print(f"âŒ Lá»—i khi post bÃ i bÃ¡o: {e}")
        return None

def update_source_last_crawled(source_id: int) -> bool:
    """Cáº­p nháº­t thá»i gian crawl cuá»‘i cho nguá»“n."""
    try:
        payload = {"last_crawled_at": datetime.now().isoformat()}
        response = requests.put(f"{API_BASE_URL}/crawl-sources/{source_id}", json=payload)
        response.raise_for_status()
        return True
    except Exception as e:
        print(f"âŒ Lá»—i khi cáº­p nháº­t nguá»“n {source_id}: {e}")
        return False

def fetch_and_process_all_active_sources():
    """Láº¥y vÃ  xá»­ lÃ½ tin tá»©c tá»« cÃ¡c nguá»“n Ä‘ang hoáº¡t Ä‘á»™ng."""
    print(f"\nğŸ• {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Báº¯t Ä‘áº§u chu ká»³ xá»­ lÃ½...")
    
    try:
        response = requests.get(f"{API_BASE_URL}/crawl-sources/?is_active=true")
        response.raise_for_status()
        sources = response.json()
        print(f"ğŸ“Š TÃ¬m tháº¥y {len(sources)} nguá»“n Ä‘ang hoáº¡t Ä‘á»™ng.")
        
        total_new_articles = 0
        
        for source in sources:
            print(f"\nğŸ” Äang crawl: {source['name']}")
            
            # 1. CRAWL TIN Tá»¨C
            scraped_articles = scrape_news_from_website(
                page_url=source['url'],
                article_container_selector=source['article_container_selector'],
                title_selector=source['title_selector'],
                link_selector=source['link_selector'],
                summary_selector=source.get('summary_selector'),
                date_selector=source.get('date_selector'),
                source_name=source['name'],
                max_articles=2
            )
            
            if not scraped_articles:
                print(f"   âš ï¸ KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t má»›i nÃ o tá»« {source['name']}")
                update_source_last_crawled(source['id'])
                continue

            # 2. LÆ¯U BÃ€I BÃO (AI sáº½ Ä‘Æ°á»£c xá»­ lÃ½ tá»± Ä‘á»™ng trong article_crud.py)
            new_articles_count_for_source = 0
            for article in scraped_articles:
                created_article = post_article_to_api(article)
                
                if created_article:
                    new_articles_count_for_source += 1
                    print(f"   ğŸ“ BÃ i viáº¿t sáº½ Ä‘Æ°á»£c phÃ¢n tÃ­ch AI tá»± Ä‘á»™ng trong backend")
            
            total_new_articles += new_articles_count_for_source
            
            # 3. Cáº¬P NHáº¬T THá»œI GIAN CRAWL CUá»I
            update_source_last_crawled(source['id'])
            
            time.sleep(2)
        
        print(f"\nğŸ‰ HoÃ n thÃ nh chu ká»³: {total_new_articles} bÃ i bÃ¡o má»›i Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½.")
        
    except Exception as e:
        print(f"âŒ Lá»—i nghiÃªm trá»ng trong chu ká»³ xá»­ lÃ½: {e}")

def check_api_connection():
    """Kiá»ƒm tra káº¿t ná»‘i API."""
    try:
        response = requests.get(f"{API_BASE_URL.replace('/api/v1', '')}/health", timeout=5)
        response.raise_for_status()
        print("âœ… API Ä‘ang hoáº¡t Ä‘á»™ng")
        return True
    except Exception as e:
        print(f"âŒ KhÃ´ng thá»ƒ káº¿t ná»‘i API: {e}")
        return False

def main():
    print("=" * 80)
    print("ğŸ¤– STOCK NEWS TRACKER SCHEDULER (with Gemini AI)")
    print("=" * 80)
    
    if not check_api_connection():
        return
        
    # Láº­p lá»‹ch
    schedule.every(15).minutes.do(fetch_and_process_all_active_sources)
    
    print("â° Scheduler Ä‘Ã£ khá»Ÿi Ä‘á»™ng. Lá»‹ch: Má»—i 15 phÃºt.")
    print("ğŸ¤– AI phÃ¢n tÃ­ch sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n tá»± Ä‘á»™ng trong backend.")
    
    # Cháº¡y ngay láº§n Ä‘áº§u Ä‘á»ƒ test
    print("\nğŸš€ Cháº¡y chu ká»³ Ä‘áº§u tiÃªn ngay bÃ¢y giá»...")
    fetch_and_process_all_active_sources()
    
    # VÃ²ng láº·p chÃ­nh
    try:
        while True:
            schedule.run_pending()
            time.sleep(60)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ÄÃ£ dá»«ng scheduler.")

if __name__ == "__main__":
    main()
