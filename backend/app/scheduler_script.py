# app/scheduler_script.py
import requests
import json
import time
import schedule
from datetime import datetime
from typing import List, Dict, Optional
import sys
import os

# ThÃªm thÆ° má»¥c backend vÃ o Python path
current_dir = os.path.dirname(os.path.abspath(__file__))
backend_dir = os.path.dirname(current_dir)
sys.path.insert(0, backend_dir)

# BÃ¢y giá» cÃ³ thá»ƒ import bÃ¬nh thÆ°á»ng
from app.services.generic_crawler import scrape_news_from_website
from setup_sample_sources import main as source_setup
from setup_watchlist import main as watchlist_setup
from setup_company import main as company_setup
from app.services.notification_service import test_telegram_connection
from app.services.financial_api_service import fetch_all_active_company_metrics

API_BASE_URL = "https://stock-news-tracker-production.up.railway.app/api/v1"

def post_article_to_api(article_data: dict) -> Optional[Dict]:
    """Gá»­i bÃ i bÃ¡o Ä‘Ã£ crawl lÃªn API Ä‘á»ƒ lÆ°u trá»¯."""
    payload = {
        "title": article_data.get("title"),
        "url": article_data.get("url"),
        "summary": article_data.get("summary"),
        "published_date_str": article_data.get("published_date_str") or article_data.get("collected_at_iso"),
        "source_url": article_data.get("source_page")
    }
    
    payload = {k: v for k, v in payload.items() if v is not None}

    try:
        response = requests.post(f"{API_BASE_URL}/articles/", json=payload)
        response.raise_for_status()
        
        created_article = response.json()
        print(f"âœ… Posted article to DB: '{payload.get('title')[:50]}...' (ID: {created_article.get('id')})")
        return created_article
        
    except Exception as e:
        print(f"âŒ Lá»—i khi post bÃ i bÃ¡o: {e}")
        return None

def update_source_last_crawled(source_id: int) -> bool:
    """Cáº­p nháº­t thá»i gian crawl cuá»‘i cho nguá»“n."""
    try:
        payload = {"last_crawled_at": datetime.now().isoformat()}
        response = requests.put(f"{API_BASE_URL}/crawl-sources/{source_id}/", json=payload)
        response.raise_for_status()
        return True
    except Exception as e:
        print(f"âŒ Lá»—i khi cáº­p nháº­t nguá»“n {source_id}: {e}")
        return False

def fetch_and_process_all_active_sources():
    """Láº¥y vÃ  xá»­ lÃ½ tin tá»©c tá»« cÃ¡c nguá»“n Ä‘ang hoáº¡t Ä‘á»™ng."""
    print(f"\nğŸ• {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Báº¯t Ä‘áº§u chu ká»³ xá»­ lÃ½...")
    
    try:
        response = requests.get(f"{API_BASE_URL}/crawl-sources/?is_active=true/")
        response.raise_for_status()
        sources = response.json()
        print(f"ğŸ“Š TÃ¬m tháº¥y {len(sources)} nguá»“n Ä‘ang hoáº¡t Ä‘á»™ng.")
        
        total_new_articles = 0
        
        for source in sources:
            print(f"\nğŸ” Äang crawl: {source['name']}")
            
            # 1. CRAWL TIN Tá»¨C
            scraped_articles = scrape_news_from_website(
                page_url=source['url'],
                article_container_selector=source['article_container_selector'],
                title_selector=source['title_selector'],
                link_selector=source['link_selector'],
                summary_selector=source.get('summary_selector'),
                date_selector=source.get('date_selector'),
                source_name=source['name'],
                max_articles=2
            )
            
            if not scraped_articles:
                print(f"   âš ï¸ KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t má»›i nÃ o tá»« {source['name']}")
                update_source_last_crawled(source['id'])
                continue

            # 2. LÆ¯U BÃ€I BÃO (AI sáº½ Ä‘Æ°á»£c xá»­ lÃ½ tá»± Ä‘á»™ng trong article_crud.py)
            new_articles_count_for_source = 0
            for article in scraped_articles:
                created_article = post_article_to_api(article)
                
                if created_article:
                    new_articles_count_for_source += 1
                    print(f"   ğŸ“ BÃ i viáº¿t sáº½ Ä‘Æ°á»£c phÃ¢n tÃ­ch AI tá»± Ä‘á»™ng trong backend")
            
            total_new_articles += new_articles_count_for_source
            
            # 3. Cáº¬P NHáº¬T THá»œI GIAN CRAWL CUá»I
            update_source_last_crawled(source['id'])
            
            time.sleep(2)
        
        print(f"\nğŸ‰ HoÃ n thÃ nh chu ká»³: {total_new_articles} bÃ i bÃ¡o má»›i Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½.")
        
    except Exception as e:
        print(f"âŒ Lá»—i nghiÃªm trá»ng trong chu ká»³ xá»­ lÃ½: {e}")

def check_api_connection():
    """Kiá»ƒm tra káº¿t ná»‘i API."""
    try:
        response = requests.get(f"{API_BASE_URL.replace('/api/v1', '')}/health/", timeout=5)
        response.raise_for_status()
        print("âœ… API Ä‘ang hoáº¡t Ä‘á»™ng")
        return True
    except Exception as e:
        print(f"âŒ KhÃ´ng thá»ƒ káº¿t ná»‘i API: {e}")
        return False

def fetch_company_metrics():
    """
    ğŸ¯ Function Ä‘Æ°á»£c gá»i bá»Ÿi scheduler
    Gá»i Ä‘áº¿n services/financial_api_service.py
    """
    print(f"\nğŸ“Š {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - SCHEDULER: Gá»i Financial API Service...")
    
    try:
        # ğŸ”¥ Gá»ŒI SERVICE THAY VÃŒ SETUP_COMPANY
        summary = fetch_all_active_company_metrics()
        
        if summary['success_count'] > 0:
            print(f"âœ… SCHEDULER: ThÃ nh cÃ´ng fetch metrics cho {summary['success_count']} companies")
            
            # Gá»­i notification náº¿u muá»‘n
            # if summary['success_count'] >= 5:
            #     notification_message = f"ğŸ“Š Company Metrics Update\nâœ… Successfully fetched metrics for {summary['success_count']} companies\nğŸ”§ API usage: {summary['api_requests_used']}/{summary['api_limit']}"
                 # notification_service.send_telegram_message_sync(notification_message)
        
        if summary['error_count'] > 0:
            print(f"âš ï¸ SCHEDULER: {summary['error_count']} companies cÃ³ lá»—i")
            
    except Exception as e:
        print(f"âŒ SCHEDULER: Lá»—i khi gá»i Financial API Service: {e}")

def gather_data():
    fetch_company_metrics()
    fetch_and_process_all_active_sources()


def main():

    print("=" * 80)
    print("ğŸ¤– STOCK NEWS TRACKER SCHEDULER (with Gemini AI)")
    print("=" * 80)

    source_setup()
    watchlist_setup()
    company_setup()
    test_telegram_connection()
    
    if not check_api_connection():
        return
        
    # Láº­p lá»‹ch
    schedule.every(1).hours.do(gather_data)
    
    print("â° Scheduler Ä‘Ã£ khá»Ÿi Ä‘á»™ng. Lá»‹ch: Má»—i 15 phÃºt.")
    print("ğŸ¤– AI phÃ¢n tÃ­ch sáº½ Ä‘Æ°á»£c thá»±c hiá»‡n tá»± Ä‘á»™ng trong backend.")
    
    # Cháº¡y ngay láº§n Ä‘áº§u Ä‘á»ƒ test
    print("\nğŸš€ Cháº¡y chu ká»³ Ä‘áº§u tiÃªn ngay bÃ¢y giá»...")
    gather_data()
    
    # VÃ²ng láº·p chÃ­nh
    try:
        while True:
            schedule.run_pending()
            time.sleep(60)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ÄÃ£ dá»«ng scheduler.")

if __name__ == "__main__":
    main()
