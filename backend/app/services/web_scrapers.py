import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Any
from datetime import datetime
import logging
import time
import re

# Cáº¥u hÃ¬nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def scrape_vnexpress_page(
    page_url: str, 
    source_name: str = "vnexpress.net"
) -> List[Dict[str, str]]:
    """
    Crawler 1 bÃ i viáº¿t má»›i nháº¥t tá»« trang VnExpress
    """
    articles = []
    
    try:
        # Headers Ä‘á»ƒ giáº£ láº­p trÃ¬nh duyá»‡t thá»±c
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
        
        # Gá»­i request vá»›i timeout
        response = requests.get(page_url, headers=headers, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        # Parse HTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # CÃ¡c selector cho VnExpress (thá»­ nhiá»u pattern)
        article_selectors = [
            ".item-news",           # Selector chÃ­nh cho bÃ i viáº¿t
            ".item-news-common",    # Selector phá»¥
            ".title-news",          # Selector cho tiÃªu Ä‘á»
            "article",              # Tháº» article
            ".story",               # Selector story
            ".item-news-wrap"       # Selector wrap
        ]
        
        article_found = None
        
        # Thá»­ tá»«ng selector cho Ä‘áº¿n khi tÃ¬m Ä‘Æ°á»£c bÃ i viáº¿t
        for selector in article_selectors:
            article_blocks = soup.select(selector)
            if article_blocks:
                article_found = article_blocks[0]  # Láº¥y bÃ i viáº¿t Ä‘áº§u tiÃªn (má»›i nháº¥t)
                break
        
        if not article_found:
            # Náº¿u khÃ´ng tÃ¬m Ä‘Æ°á»£c báº±ng selector, thá»­ tÃ¬m link Ä‘áº§u tiÃªn cÃ³ class title
            title_links = soup.select("h3 a, h2 a, h1 a, .title a, .title-news a")
            if title_links:
                article_found = title_links[0].parent
        
        if article_found:
            try:
                # TrÃ­ch xuáº¥t tiÃªu Ä‘á»
                title_selectors = ["h3 a", "h2 a", "h1 a", ".title a", ".title-news a", "a"]
                title = ""
                url = ""
                
                for title_sel in title_selectors:
                    title_element = article_found.select_one(title_sel)
                    if title_element:
                        title = title_element.get_text(strip=True)
                        url = title_element.get('href', '')
                        if title and url:
                            break
                
                # Náº¿u váº«n khÃ´ng cÃ³ tiÃªu Ä‘á», thá»­ tÃ¬m trá»±c tiáº¿p trong trang
                if not title:
                    title_element = soup.select_one("h1, .title-detail, .title-news")
                    if title_element:
                        title = title_element.get_text(strip=True)
                        url = page_url  # Sá»­ dá»¥ng URL gá»‘c
                
                # Bá» QUA náº¿u khÃ´ng cÃ³ tiÃªu Ä‘á»
                if not title or title.strip() == "":
                    logger.info(f"Bá» qua: KhÃ´ng cÃ³ tiÃªu Ä‘á» tá»« {source_name}")
                    return articles
                
                # Xá»­ lÃ½ URL
                if url.startswith('/'):
                    url = f"https://vnexpress.net{url}"
                elif not url.startswith('http'):
                    url = page_url
                
                # TrÃ­ch xuáº¥t tÃ³m táº¯t - Cáº¢I THIá»†N SELECTOR CHO VNEXPRESS
                summary = ""
                summary_selectors = [
                    ".description",         # Selector chÃ­nh cho tÃ³m táº¯t VnExpress
                    ".sapo",               # Selector sapo
                    ".lead",               # Lead paragraph
                    ".summary",            # Summary
                    ".item-news .description",  # Description trong item-news
                    ".excerpt",            # Excerpt
                    "p.description",       # Paragraph description
                    ".intro",              # Intro
                    ".abstract"            # Abstract
                ]
                
                for summary_sel in summary_selectors:
                    summary_element = article_found.select_one(summary_sel)
                    if summary_element:
                        summary = summary_element.get_text(strip=True)
                        if summary and len(summary) > 20:  # Chá»‰ láº¥y tÃ³m táº¯t cÃ³ Ã½ nghÄ©a
                            break
                
                # Náº¿u váº«n khÃ´ng cÃ³ summary, thá»­ tÃ¬m paragraph Ä‘áº§u tiÃªn
                if not summary:
                    paragraphs = article_found.select("p")
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if text and len(text) > 30 and not text.startswith("Theo"):
                            summary = text
                            break
                
                # Táº¡o dictionary cho bÃ i viáº¿t
                article_data = {
                    'title': title,
                    'url': url,
                    'summary': summary,
                    'source_page': source_name,
                    'collected_at_iso': datetime.now().isoformat()
                }
                
                articles.append(article_data)
                
                # PRINT ra bÃ i viáº¿t tÃ¬m Ä‘Æ°á»£c
                print(f"\nğŸ“° BÃ i viáº¿t tá»« {source_name}:")
                print(f"   TiÃªu Ä‘á»: {title}")
                print(f"   URL: {url}")
                if summary:
                    print(f"   TÃ³m táº¯t: {summary[:100]}...")
                else:
                    print(f"   TÃ³m táº¯t: KhÃ´ng cÃ³")
                print(f"   Thá»i gian: {article_data['collected_at_iso']}")
                print("-" * 80)
                
            except Exception as e:
                logger.error(f"Lá»—i khi xá»­ lÃ½ bÃ i viáº¿t tá»« {source_name}: {str(e)}")
        
        else:
            logger.warning(f"KhÃ´ng tÃ¬m tháº¥y bÃ i viáº¿t nÃ o tá»« {source_name}")
        
        # Delay Ä‘á»ƒ trÃ¡nh bá»‹ block
        time.sleep(1)
        
    except requests.RequestException as e:
        logger.error(f"Lá»—i káº¿t ná»‘i khi crawler {source_name}: {str(e)}")
    except Exception as e:
        logger.error(f"Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh khi crawler {source_name}: {str(e)}")
    
    return articles

def scrape_all_vnexpress_news() -> List[Dict[str, str]]:
    """
    Crawler 1 bÃ i viáº¿t má»›i nháº¥t tá»« má»—i nguá»“n VnExpress
    """
    all_articles = []
    
    # Danh sÃ¡ch cÃ¡c nguá»“n theo yÃªu cáº§u
    sources = [
        # ğŸ¢ 1. Tin tá»©c doanh nghiá»‡p
        {
            "url": "https://vnexpress.net/kinh-doanh/doanh-nghiep",
            "name": "VnExpress - Doanh nghiá»‡p"
        },
        
        # ğŸ“Š 2. Chá»‰ sá»‘ tÃ i chÃ­nh vÃ  hÆ°á»›ng dáº«n Ä‘áº§u tÆ°
        {
            "url": "https://vnexpress.net/chi-so-p-b-p-e-la-gi-4296226.html",
            "name": "VnExpress - Chá»‰ sá»‘ P/B, P/E"
        },
        {
            "url": "https://vnexpress.net/chi-so-eps-la-gi-4461207.html",
            "name": "VnExpress - Chá»‰ sá»‘ EPS"
        },
        {
            "url": "https://vnexpress.net/roa-roe-la-gi-4304366.html",
            "name": "VnExpress - ROA, ROE"
        },
        {
            "url": "https://vnexpress.net/meo-doc-bao-cao-tai-chinh-danh-cho-dau-tu-chung-khoan-4862943.html",
            "name": "VnExpress - BÃ¡o cÃ¡o tÃ i chÃ­nh"
        },
        
        # ğŸŒ 3. ChÃ­nh sÃ¡ch kinh táº¿ vÃ  Ä‘á»‹a chÃ­nh trá»‹
        {
            "url": "https://vnexpress.net/chu-de/chinh-sach-kinh-te-1073",
            "name": "VnExpress - ChÃ­nh sÃ¡ch kinh táº¿"
        },
        {
            "url": "https://vnexpress.net/chu-de/luat-doanh-nghiep-7163",
            "name": "VnExpress - Luáº­t doanh nghiá»‡p"
        },
        {
            "url": "https://vnexpress.net/kinh-doanh/quoc-te",
            "name": "VnExpress - Kinh doanh quá»‘c táº¿"
        },
        
        # ğŸ’° 4. GiÃ¡ vÃ ng vÃ  tá»· giÃ¡ USD
        {
            "url": "https://vnexpress.net/chu-de/gia-vang-1403",
            "name": "VnExpress - GiÃ¡ vÃ ng"
        },
        {
            "url": "https://vnexpress.net/tag/gia-usd-267904",
            "name": "VnExpress - GiÃ¡ USD"
        },
        
        # ğŸ” 5. CÃ¡c chuyÃªn má»¥c má»Ÿ rá»™ng khÃ¡c
        {
            "url": "https://vnexpress.net/kinh-doanh",
            "name": "VnExpress - Kinh doanh"
        },
        {
            "url": "https://vnexpress.net/tag/chinh-sach-tien-te-1044604",
            "name": "VnExpress - ChÃ­nh sÃ¡ch tiá»n tá»‡"
        }
    ]
    
    print(f"\nğŸ” Báº¯t Ä‘áº§u crawler {len(sources)} nguá»“n tá»« VnExpress...")
    
    for idx, source in enumerate(sources, 1):
        print(f"\nâ³ [{idx}/{len(sources)}] Äang crawler: {source['name']}...")
        
        articles = scrape_vnexpress_page(source['url'], source['name'])
        all_articles.extend(articles)
        
        # Delay giá»¯a cÃ¡c request
        time.sleep(2)
    
    print(f"\nğŸ‰ Tá»”NG Káº¾T: ÄÃ£ crawler {len(all_articles)} bÃ i viáº¿t tá»« {len(sources)} nguá»“n VnExpress")
    return all_articles

def scrape_all_news() -> List[Dict[str, str]]:
    """
    HÃ m chÃ­nh Ä‘á»ƒ crawler táº¥t cáº£ tin tá»©c
    """
    return scrape_all_vnexpress_news()

# Test function
if __name__ == "__main__":
    print("=" * 120)
    print("ğŸš€ Báº®T Äáº¦U CRAWLER TIN Tá»¨C VNEXPRESS (1 BÃ€I Má»šI NHáº¤T Má»–I NGUá»’N)")
    print("=" * 120)
    
    # Test crawler tin tá»©c
    articles = scrape_all_news()
    
    print(f"\nğŸ“Š Káº¾T QUáº¢ CUá»I CÃ™NG:")
    print(f"   Tá»•ng sá»‘ bÃ i viáº¿t: {len(articles)}")
    
    # Thá»‘ng kÃª theo nguá»“n
    sources = {}
    for article in articles:
        source = article['source_page']
        sources[source] = sources.get(source, 0) + 1
    
    print(f"\nğŸ“‹ THá»NG KÃŠ THEO NGUá»’N:")
    for source, count in sources.items():
        print(f"   âœ… {source}: {count} bÃ i viáº¿t")
    
    # **THÃŠM SUMMARY VÃ€O DANH SÃCH BÃ€I VIáº¾T**
    print(f"\nğŸ“ DANH SÃCH Táº¤T Cáº¢ BÃ€I VIáº¾T (CÃ“ SUMMARY):")
    for idx, article in enumerate(articles, 1):
        print(f"\n{idx}. {article['title']}")
        print(f"   ğŸ”— URL: {article['url']}")
        print(f"   ğŸ“° Nguá»“n: {article['source_page']}")
        if article['summary']:
            print(f"   ğŸ“„ TÃ³m táº¯t: {article['summary']}")
        else:
            print(f"   ğŸ“„ TÃ³m táº¯t: KhÃ´ng cÃ³")
        print(f"   â° Thá»i gian: {article['collected_at_iso']}")
        print("-" * 100)
